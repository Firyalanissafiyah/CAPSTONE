# -*- coding: utf-8 -*-
"""another copy of ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12YpuSCwmY7G0fxc-iBy5ek63LoXQs_0m

#**TripMate**

## **1. Import Library**
"""

# For data processing
import pandas as pd
import numpy as np

# For data visualization
import seaborn as sns
import matplotlib.pyplot as plt

# For modelling
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""##**2. Upload Dataset**"""

# URL of raw file dataset in GitHub repository
place = 'https://raw.githubusercontent.com/Firyalanissafiyah/CAPSTONE/main/dataset/tourism_with_id.csv'
rating = 'https://raw.githubusercontent.com/Firyalanissafiyah/CAPSTONE/main/dataset/tourism_rating.csv'
user = 'https://raw.githubusercontent.com/Firyalanissafiyah/CAPSTONE/main/dataset/user.csv'

"""##**3. Data Preprocessing**

**a. Place**
"""

# Load data 'place' from URL to DataFrame
place = pd.read_csv(place)

# Viewing overview of the 'place' data
place.head()

# Dropping unused columns
place = place.drop(['Unnamed: 11','Unnamed: 12', 'Time_Minutes', 'Coordinate', 'Lat', 'Long'], axis=1)
place.head(3)

# Delete rows that have a value of 0 in the Price column
place = place[place['Price'] != 0]
place.head(10)

# Information about the DataFrame 'place'
place.info()

"""**b. Rating**"""

# Load 'rating' data from URL to DataFrame
rating = pd.read_csv(rating)

# Viewing overview of the rating data
rating.head(3)

# Information about the DataFrame 'rating'
rating.info()

# Filtering the rating data to include only ratings for tourist attractions
place_rating = pd.merge(rating, place[['Place_Id', 'Category', 'City', 'Price', 'Place_Name']], on='Place_Id', how='left')
place_rating

# Information about shape of 'place_rating' DataFrame
place_rating.shape

# Information about the DataFrame 'place_rating'
place_rating.info()

"""**c. User**"""

# Load data 'user' from URL to DataFrame
user = pd.read_csv(user)

# Viewing overview of the user data
user.head()

# Viewing the dataset of users who have rated tourist attractions in Bandung City
user.shape

"""##**4. Exploratory Data**"""

# Create a visualization of the number of tourist attractions rating
count = place_rating['Place_Ratings'].value_counts()
percent = 100 * place_rating['Place_Ratings'].value_counts(normalize = True)
df = pd.DataFrame({'Jumlah rating' : count, 'Persentase' : percent.round(1)})
print(df)
sns.countplot(x = place_rating['Place_Ratings'], data = place_rating, order = place_rating['Place_Ratings'].value_counts().index)
plt.title('Jumlah Rating Tempat Wisata', pad=15)
plt.xlabel('Rating')
plt.ylabel('Jumlah User')
plt.show()

# Create a visualization of the number of tourist categories based on the number of ratings
count = place_rating['Category'].value_counts()
percent = 100 * place_rating['Category'].value_counts(normalize = True)
df = pd.DataFrame({'Jumlah rating' : count, 'Persentase' : percent.round(1)})
print(df)
sns.countplot(y = place_rating['Category'], data = place_rating, order = place_rating['Category'].value_counts().index)
plt.title('Perbandingan Jumlah Kategori Wisata', pad=15)
plt.xlabel('Jumlah Wisata')
plt.ylabel('Kategori')
plt.show()

# Create a visualization of user age distribution
plt.figure(figsize=(5,3))
sns.boxplot(user['Age']);
plt.title('Distribusi Usia User', pad=15)
plt.ylabel('Usia')
plt.show()

# Create a visualization of tourist attraction price distribution
plt.figure(figsize=(7,3))
sns.boxplot(place_rating['Price'])
plt.title('Distribusi Tiket Masuk Wisata', pad=15)
plt.ylabel('Harga')
plt.show()

# Filtering the city origin of the user
asal_kota = user['Location'].apply(lambda x : x.split(',')[0])

# Visualization of the city of origin of the user
plt.figure(figsize=(8,6))
sns.countplot(y=asal_kota)
plt.title('Jumlah Asal Kota dari User')
plt.xlabel('Jumlah')
plt.ylabel('Asal Kota')
plt.show()

"""## **5. Data Preparation**"""

# Check null value
place_rating.isnull().sum()

# Delete rows that have NaN values in the 'Place_Name' column
place_rating = place_rating.dropna(subset=['Place_Name'])

# Count the number of Place_Id in place_rating dataset
len(place_rating.Place_Id.unique())

# Read dataset for encoding
tourism = place_rating.copy()
tourism.head()

# Delete duplicates of tourism dataset
tourism = tourism.drop_duplicates('Place_Id')
tourism

# Create a new variable for dictianory
place_id = tourism['Place_Id'].tolist()
place_category = tourism['Category'].tolist()
place_name = tourism['Place_Name'].tolist()
price = tourism['Price'].tolist()
rating = tourism['Place_Ratings'].tolist()
city = tourism['City'].tolist()

print(len(place_id))
print(len(place_category))
print(len(place_name))
print(len(price))
print(len(rating))
print(len(city))

# Create a place_recommend dictianory
place_recommend = pd.DataFrame({
    'id' : place_id,
    'place_category' : place_category,
    'place_name' : place_name,
    'price' : price,
    'rating' : rating,
    'city' : city
})

place_recommend

"""## **6. Modelling**

#### **a. Content Based Filtering**
"""

# Make a copy of the DataFrame
all_data = place_recommend
all_data.head()

# Convert text to numerical representation
tf = TfidfVectorizer()
tf.fit(all_data['place_name'])
tf.get_feature_names_out()

# Convert data into integer matrix form
tfdif_matrix = tf.fit_transform(all_data['place_name'])
tfdif_matrix.shape

# Convert tf-dif vector into matrix form
tfdif_matrix.todense()

# Sampling and inspecting a subset of the TF-IDF matrix for place names and categories.
pd.DataFrame(
    tfdif_matrix.todense(),
    columns = tf.get_feature_names_out(),
    index = all_data.place_category
).sample(22, axis = 1).sample(10, axis = 0)

"""**COSINE SIMILARITY**"""

# Cosine similarity matrix calculation
cosine_sim = cosine_similarity(tfdif_matrix)
cosine_sim

# Cosine similarity DataFrame creation
cosine_sim_df = pd.DataFrame(cosine_sim, index = all_data['place_name'], columns = all_data['place_name'])
print('Shape : ', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis = 1).sample(10, axis = 0)

# Function for recommendation
def place_recommendations(nama_tempat, similarity_data = cosine_sim_df, items = all_data[['place_name', 'place_category']], k = 5) :
  index = similarity_data.loc[:, nama_tempat].to_numpy().argpartition(
      range(-1, -k, -1)
  )

  closest = similarity_data.columns[index[-1:-(k+2):-1]]
  closest = closest.drop(nama_tempat, errors = 'ignore')

  return pd.DataFrame(closest).merge(items).head(k)

# Example of filtering data for specific place
all_data[all_data.place_name.eq('Candi Gedong Songo')]

place_recommendations('Candi Gedong Songo')

"""#### **2. Collaborative Filtering**"""

# Sorting tourism data by User ID
trip = tourism
trip = trip.sort_values(by='User_Id', ascending=True)
trip

# Encoding user IDs in Tourism data
user_ids = trip['User_Id'].unique().tolist()
print('list User_Id : ', user_ids)

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded User_Id : ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke User_Id : ', user_encoded_to_user)

# Encoding place IDs in Tourism data
place_ids = trip['Place_Id'].unique().tolist()
print('list Place_Id : ', place_ids)

place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}
print('encoded Place_Id : ', place_to_place_encoded)

place_encoded_to_place = {i: x for i, x in enumerate(place_ids)}
print('encoded angka ke Place_Id : ', place_encoded_to_place)

# Mapping User_Id dan Place_Id ke indeks numerik
trip['user'] = trip['User_Id'].map(user_to_user_encoded)
trip['place'] = trip['Place_Id'].map(place_to_place_encoded)

# Calculating the number of users, rlaces, and rating range
num_users = len(user_to_user_encoded)
print(num_users)

num_places = len(place_encoded_to_place)
print(num_places)

trip['Place_Ratings'] = trip['Place_Ratings'].values.astype(np.float32)

min_rating = min(trip['Place_Ratings'])

max_rating = max(trip['Place_Ratings'])

print('Number of User : {}, Number of Place : {}, Min Rating {}, Max Rating {}'.format(
    num_users, num_places, min_rating, max_rating
))

"""## **7. Training and Validation Data**"""

# Shuffling the trip DataFrame
trip = trip.sample(frac = 1, random_state = 30)
trip

# Viewing information about trip
trip.info()

# Preparing training and validation data
x = trip[['User_Id', 'Place_Id']].values

y = trip['Place_Ratings'].apply(lambda x: (x - min(trip['Place_Ratings'])) / (max(trip['Place_Ratings']) - min(trip['Place_Ratings']))).values

train_indices = int(0.8 * trip.shape[0])
x_train, x_val, y_train, y_val = (x[:train_indices], x[train_indices:], y[:train_indices], y[train_indices:])

print(x, y)

# Calculate the maximum indices for users and places
max_user_index = trip['User_Id'].max()
max_place_index = trip['Place_Id'].max()

# Use the max indices to set the dimensions for embedding layers
num_users = max_user_index + 1
num_places = max_place_index + 1
embedding_size = 50

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_places, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_places = num_places
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.places_embedding = layers.Embedding(
            num_places,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.places_bias = layers.Embedding(num_places, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        places_vector = self.places_embedding(inputs[:, 1])
        places_bias = self.places_bias(inputs[:, 1])

        dot_user_places = tf.tensordot(user_vector, places_vector, 2)

        x = dot_user_places + user_bias + places_bias

        return tf.nn.sigmoid(x)


num_users = 1000
num_places = 1000
embedding_size = 50

model = RecommenderNet(num_users, num_places, 50)

model.compile(
    loss = 'binary_crossentropy',
    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),
    metrics = [tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size=16,
    epochs = 100,
    validation_data = (x_val, y_val),
)

"""## **8. Visualization**"""

# Plotting model training and validation RMSE over epochs
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model')
plt.ylabel('RMSE')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc = 'upper left')
plt.show()

"""## **9. System Recommendation**"""

# Ensure required variables and model are defined
assert 'place_recommend' in globals(), "Variable 'place_recommend' is not defined"
assert 'place_to_place_encoded' in globals(), "Variable 'place_to_place_encoded' is not defined"
assert 'user_to_user_encoded' in globals(), "Variable 'user_to_user_encoded' is not defined"
assert 'model' in globals(), "Model is not defined"

place_trip = place_recommend
trip = pd.read_csv('https://raw.githubusercontent.com/Firyalanissafiyah/CAPSTONE/main/dataset/tourism_rating.csv')

trip['Place_Ratings'] = trip['Place_Ratings'].values.astype(np.float32)

# Sampling a user_id and verifying it
user_id = trip.User_Id.sample(1).iloc[0]
print(f'Selected user_id: {user_id}')

user_encoder = user_to_user_encoded.get(user_id)
while user_encoder is None:
    user_id = trip.User_Id.sample(1).iloc[0]
    user_encoder = user_to_user_encoded.get(user_id)
print(f'New selected user_id: {user_id}')

place_visited_by_user = trip[trip.User_Id == user_id]
print(f'Places visited by user: {place_visited_by_user}')

place_not_visited = place_trip[~place_trip['id'].isin(place_visited_by_user.Place_Id.values)]['id']
place_not_visited = list(
    set(place_not_visited)
    .intersection(set(place_to_place_encoded.keys()))
)
print(f'Place not visited: {place_not_visited}')

# Encoding place_not_visited and checking for None
place_not_visited = [[place_to_place_encoded.get(x)] for x in place_not_visited]
print(f'Encoded places not visited: {place_not_visited}')

# Checking if any place encoding resulted in None
for place in place_not_visited:
    if place[0] is None:
        raise ValueError("One of the place IDs could not be encoded properly")

# Creating user_place_array and checking its contents
user_place_array = np.hstack(
    ([[user_encoder]] * len(place_not_visited), place_not_visited)
)
print(f'user_place_array: {user_place_array}')

rate = model.predict(user_place_array).flatten()

top_rate_indices = rate.argsort()[-8:][::-1]
recommended_place_ids = [
    place_encoded_to_place.get(place_not_visited[x][0]) for x in top_rate_indices
]

print('Rekomendasi untuk user : {}'.format(user_id))
print('====' * 10)
print('Tempat dengan rating wisata user paling tinggi')
print('----' * 10)

top_place_user = (
    place_visited_by_user.sort_values(
        by = 'Place_Ratings',
        ascending=False
    )
    .head(5)
    .Place_Id.values
)

place_df_rows = place_trip[place_trip['id'].isin(top_place_user)]
for row in place_df_rows.itertuples():
  print(row.place_name, ':', row.place_category)

print('----' * 10)
print('Top 8 Rekomendasi Wisata')
print('----' * 10)

recommended_place = place_trip[place_trip['id'].isin(recommended_place_ids)]
for row in recommended_place.itertuples():
  print(row.place_name, ':', row.place_category)
print('===' * 15)

# Saving the model
model.save("model_tripmate", save_format="tf")

rate = model.predict(user_place_array).flatten()

top_rate_indices = rate.argsort()[-8:][::-1]
recommended_place_ids = [
    place_encoded_to_place.get(place_not_visited[x][0]) for x in top_rate_indices
]

print('Rekomendasi untuk user : {}'.format(user_id))
print('====' * 10)
print('Tempat dengan rating wisata user paling tinggi')
print('----' * 10)

top_place_user = (
    place_visited_by_user.sort_values(
        by='Place_Ratings',
        ascending=False
    )
    .head(5)
    .Place_Id.values
)

place_df_rows = place_trip[place_trip['id'].isin(top_place_user)]
for row in place_df_rows.itertuples():
    print(row.place_name, ':', row.place_category)

print('----' * 10)
print('Top 8 Rekomendasi Wisata')
print('----' * 10)

recommended_place = place_trip[place_trip['id'].isin(recommended_place_ids)]

# Checking the presence of 'place_price' and 'place_rating' columns
has_place_price = 'place_price' in place_trip.columns
has_place_rating = 'place_rating' in place_trip.columns

for row in recommended_place.itertuples():
    if has_place_price and has_place_rating:
        print(row.place_name, ':', row.place_category, ':', row.place_price, ':', row.place_rating)
    elif has_place_price:
        print(row.place_name, ':', row.place_category, ':', row.place_price)
    elif has_place_rating:
        print(row.place_name, ':', row.place_category, ':', row.place_rating)
    else:
        print(row.place_name, ':', row.place_category)
print('===' * 15)

model.save_weights("model_tripmate_weights.h5")